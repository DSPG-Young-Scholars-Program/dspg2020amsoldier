---
title: "2020 DSPG Symposium Presentation"
description: "Symposium Presentation of the 2020 DSPG American Soldier Project"
weight: 1
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE);
knitr::opts_chunk$set(warning = FALSE, message = FALSE) ;
# load necessary libraries
library(tidytext)
library(ggplot2)
# to install ggradar, run the line commented out below
# devtools::install_github("ricardo-bion/ggradar", dependencies = TRUE)
library(ggradar)
library(tibble)
library(scales)
library(fmsb)
library(data.table)
library(tidyverse)
library(here)
library(wordcloud)
library(SnowballC)
library(igraph)
library(ggraph)
library("RPostgreSQL")
library(stringi)
library(textstem)
library(dplyr)
library(tidyr)
library(widyr)
library(stringr)
library(BTM)
library(udpipe)
library(networkD3)
```

```{r, include=FALSE}
source(here::here("src", "load_data.R"));
source(here::here("src", "sentiment_analysis.R"));
source(here::here("src", "word_selection.R"))


colors <- c("#e57200", "#232d4b");
```

```{css, echo=FALSE}
/* this chunk of code centers all of the headings */
h1, h2, h3, h4 {
  text-align: center;
}
```

```{r, include=FALSE}
remove_words <- function(text, words) {
  pattern <- paste(words, collapse = "|");
  text <- str_replace_all(text, pattern, "");
  return(text);
}

get_nrc_sentiments <- function(data) {
  # tokenize and join with nrc sentiment lexicon
  tokens <- data %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments("nrc"));

  # compute sentiments
  sentiments <- tokens %>%
    group_by(index, racial_group, response_type, outfits, sentiment) %>%
    count() %>%
    spread(sentiment, n, fill = 0);
 
   # normalize sentiments
  sentiments <- sentiments %>%
    mutate(word_count = anger + anticipation + disgust + fear + joy + negative + positive + sadness + surprise + trust) %>%
    filter(word_count > 0) %>%
    mutate(anger = anger / word_count,
           anticipation = anticipation / word_count,
           disgust = disgust / word_count,
           fear = fear / word_count,
           joy = joy / word_count,
           negative = negative / word_count,
           positive = positive / word_count,
           sadness = sadness / word_count,
           surprise = surprise / word_count,
           trust = trust / word_count);
  return(sentiments);
};

radar <- function(sentiments, race, res_type) {
  group_mean <- dplyr::as_data_frame(sentiments) %>%
  filter(racial_group == race & response_type == res_type) %>%
  select(c("anger",
           "anticipation",
           "disgust",
           "fear",
           "joy",
           "negative",
           "positive",
           "sadness",
           "surprise",
           "trust")) %>%
  summarise_all(mean)

group_mean_melted <- melt(group_mean)
plot_data <- rbind(rep(max(group_mean_melted$value), 10), rep(min(group_mean_melted$value), 10), group_mean);

radarchart(plot_data,
           cglcol = "grey",
           cglty = 1);
}

radar2 <- function(sentiments, group1, group2, title = "Sentiment Analysis Results") {
  group1_mean <- dplyr::as_data_frame(sentiments) %>%
  filter(racial_group == group1[1] & response_type == group1[2]) %>%
  select(c("anger",
           "anticipation",
           "disgust",
           "fear",
           "joy",
           "negative",
           "positive",
           "sadness",
           "surprise",
           "trust")) %>%
  summarise_all(mean)
  
  group2_mean <- dplyr::as_data_frame(sentiments) %>%
  filter(racial_group == group2[1] & response_type == group2[2]) %>%
  select(c("anger",
           "anticipation",
           "disgust",
           "fear",
           "joy",
           "negative",
           "positive",
           "sadness",
           "surprise",
           "trust")) %>%
  summarise_all(mean)
  
  # combine repsonses
  groups <- rbind(group1_mean, group2_mean)
  rownames(groups) <- c(group1[3], group2[3])
  
  # get min and max for plotting
  groups_melted <- melt(groups)
  minval <- min(groups_melted$value)
  maxval <- max(groups_melted$value)
  
  plot_data <- rbind(rep(maxval, 10), rep(minval, 10), groups)
  
  colors <- c("#e57200", "#232d4b")
  
  radarchart(plot_data,
             cglcol = "grey", # color of net
             cglty = 1, # net line type
             pcol = colors, # line color
             cglwd = 1, # net width,
             plwd = 3, # line width
             plty = 1, # plot line type
  )
  legend(x= 1, y= 1, legend = rownames(plot_data)[-c(1,2)], bty = "n", pch = 20, col = colors );
  title(main = title);
}

visualize_bigrams <- function(bigrams, title) {
  set.seed(2020)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void() + theme(legend.position = "none",
                      plot.title = element_text(hjust = 0.5)) +
    ggtitle(title)
}
```
```{r, include = F}
tidy_topic_probs = function(model){
  df <- cbind(source = rownames(model$phi), model$phi)
  rownames(df) <- 1:nrow(df)
  df = as.data.frame(df)
  edge_list = reshape2::melt(df, id.vars=c("source"), variable.name = "target", value.name = "weight")
}
biterms_n = read.csv(here::here("data","biterms_n.csv"))

biterms_78 = read.csv(here::here("data","biterms_78.csv"))

traindata_n = read.csv(here::here("data","traindata_n.csv"))

traindata_78 = read.csv(here::here("data","traindata_78.csv"))

row.names(traindata_n) <- traindata_n$X

row.names(traindata_78) <- traindata_78$X

K = 6
modeln      <- BTM(traindata_n[-1], biterms = biterms_n[-1], k = K, iter = 2000, background = TRUE, trace = 100)

model78     <- BTM(traindata_78[-1], biterms = biterms_78[-1], k = K, iter = 2000, background = TRUE, trace = 100)
```
```{r, include = FALSE, echo=F}
V = c("V2","V3", "V4", "V5", "V6", "V7", "V8", "V9", "V10")
topics = c("Topic 1","Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", "Topic 8", "Topic 9")

topics_n_btm = tidy_topic_probs(modeln)
topics_n_btm$weight = as.numeric(topics_n_btm$weight)
topics_n_btm$target = topics_n_btm$target %>%
  plyr::mapvalues(from = V, to = topics)
topics_n_btm = topics_n_btm[, c(2, 1, 3)]

topics_78_btm = tidy_topic_probs(model78)
topics_78_btm$weight = as.numeric(topics_78_btm$weight)
topics_78_btm$target = topics_78_btm$target %>%
  plyr::mapvalues(from = V, to = topics)
topics_78_btm = topics_78_btm[, c(2, 1, 3)]


colnames(topics_n_btm) = colnames(topics_78_btm) =  c("source", "target", "weight")
```


### Overview

The American Soldier project is a digital history project funded by the National Endowment of Humanities and is led by Ed Gitre of Virginia Tech. The goal of the overall project is to take a range of surveys and commentaries by WW2 soldiers and a wide range of topics from race and gender relations, career plans, thoughts on South East Asia, and much more. Along with Ed, we have Mark Embree of Virginia Tech leading an effort in using Machine Learning to unpack and understand all the text data generated by the soldiers. Our teamâ€™s contribution to the project is to understand race relations, gender relations, and the interaction between race and space via a collection of surveys touching upon these issues as well as commentaries by the soldiers which were transcribed by citizen scientists on the Zooniverse platform. 

To approach these topics, we specifically looked at Survey 32, 144, and 190 which contain multiple choice questions ranging from demographic information, race and gender relations, thoughts on the army, war, and career plans. 

Survey 32 was administered to both black and white soldiers in order to studying attitudes of and towards black soldiers. A question of interest on the survey is, "Do you think white and black soldiers should be in separate outfits?" The soldiers gave a response to this as a multiple choice question and we have 7442 responses from black soldiers and 4678 from white soldeiors. The soldiers also had room to elaborate on their responses in a short answer field. Along with this short response, there was room for a longer comment on the overall survey. Unfortunately, we do not have any short responses to the question from black soldiers. Overall, we have 3464 written responses from black soldiers and 2324 written responses of white soldiers. The responeses themselves were on the shorter side with the average response length being shorter than 100 words.

### Approach

Our analysis was completed using three different tools: sentiment analysis, text networks, and topic modeling. These three tools help us glean insights into the feelings of soldiers, the frequencies of terms they used, and which overall topics they discussed. The text analysis is completed on Survey 32 text data and separates soldiers into four main groups of text: black soldiers' long responses, white soldiers' long responses, pro-segregation white soldier's short responses, and anti-segregation white soldier's short responses. Black and white soldiers' long responses are compared and pro- and anti-segregation white soldiers' short responses are compared.

For the sentiment analysis, we used a slightly modified version of the [NRC dictionary](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) which is a library of over 14,000 words and their associated values with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). 

### Race Relations

```{r, include=FALSE}
tot_black_words <- sum(unique_black_words$n)
tot_white_words <- sum(unique_white_words$n)

unique_black_words <- unique_black_words %>% mutate(prop = n / tot_black_words)
unique_white_words <- unique_white_words %>% mutate(prop = n / tot_white_words)

unique_black_words$color <- rep(colors[1], nrow(unique_black_words))
unique_white_words$color <- rep(colors[2], nrow(unique_white_words))

unique_seg_words$color <- rep(colors[1], nrow(unique_seg_words))
unique_int_words$color <- rep(colors[2], nrow(unique_int_words))

unique_words <- rbind(unique_black_words, unique_white_words) %>% 
  arrange(desc(prop)) %>%
  mutate(n = n / 10)
```

```{r, fig.show='hold', fig.height=8, out.width="50%", echo=FALSE}
unique_black_words %>% with(wordcloud(word, n, random.order = FALSE, colors=unique_black_words$color, max.words = 20, ordered.colors = TRUE))

unique_white_words %>% with(wordcloud(word, n, random.order = FALSE, colors = unique_white_words$color, max.words = 20, ordered.colors = TRUE))
```

```{r, echo=FALSE, fig.align='center'}
group1_mean <- unique_black_words_sentiments
group2_mean <- unique_white_words_sentiments

# combine repsonses
groups <- rbind(group1_mean, group2_mean)
rownames(groups) <- c("black", "white")

# get min and max for plotting
groups_melted <- melt(groups)
minval <- min(groups_melted$value)
maxval <- max(groups_melted$value)

plot_data <- rbind(rep(maxval, 10), rep(minval, 10), groups)

colors <- c("#e57200", "#232d4b")

radarchart(plot_data,
           cglcol = "grey", # color of net
           cglty = 1, # net line type
           pcol = colors, # line color
           cglwd = 1, # net width,
           plwd = 3, # line width
           plty = 1, # plot line type
)
legend(x=1, y=1, legend = rownames(plot_data)[-c(1,2)], bty = "n", pch = 20, col = colors );
title(main = "Sentiment Distribution of Words Used\nUniquely by Black and White Soldiers");
```

#### Topic Model Network for Black Soldiers' Long Comment

```{r black btm, fig.height=6, echo = F}
num_clusters = 6
edgelist_n_btm= topics_n_btm %>%
  filter(weight >= .01) %>%
  arrange(target)

sources <- edgelist_n_btm$source
targets <- edgelist_n_btm$target
node_names <- factor(unique(c(sort(unique(as.character(sources))), as.character(targets))))



groups = edgelist_n_btm %>% group_by(target) %>% top_n(1, weight)
groups = groups$source
nodes <- data.frame(name = node_names, group = c(1:num_clusters, groups), size = 8)
links <- data.frame(source = match(sources, node_names) - 1, 
                    target = match(targets, node_names) - 1, 
                    value = edgelist_n_btm$weight)
net_n_btm = forceNetwork(Links = links, Nodes = nodes, Source = "source",
             Target = "target", Value = "value", NodeID = "name",
             Group = "group", opacity = 0.9, zoom = T, charge = -10, fontFamily = "sans-serif", fontSize = 30)
net_n_btm
```

#### Topic Model Network for White Soldiers' Long Comment

```{r 78 btm, fig.height=6, echo = F}
edgelist_78_btm= topics_78_btm %>%
  filter(weight >= .01) %>%
  arrange(target)

sources <- edgelist_78_btm$source
targets <- edgelist_78_btm$target
node_names <- factor(unique(c(sort(unique(as.character(sources))), as.character(targets))))



groups = edgelist_78_btm %>% group_by(target) %>% top_n(1, weight)
groups = groups$source
nodes <- data.frame(name = node_names, group = c(1:num_clusters, groups), size = 8)
links <- data.frame(source = match(sources, node_names) - 1, 
                    target = match(targets, node_names) - 1, 
                    value = edgelist_78_btm$weight)
net_78_btm = forceNetwork(Links = links, Nodes = nodes, Source = "source",
             Target = "target", Value = "value", NodeID = "name",
             Group = "group", opacity = 0.9, zoom = T, charge = -10, fontFamily = "sans-serif", fontSize = 30)
net_78_btm
```




### Gender Relations

```{r, fig.align='center', echo=FALSE}
radar2(s32_female_sentiments,
       group1 = c("black", "long", "black"),
       group2 = c("white", "long", "white"), 
       title = "Average Sentiments for Black and White Soliders'\nLong Responses that Discuss Women");
```

```{r, fig.align='center', echo=FALSE}
gender_words <- fread("~/git/dspg2020amsoldier/data/dictionary/gender.csv", sep = ",") 
female_words <- gender_words %>% filter(category == "female" | category == "relation") %>% add_row(gender = "like", category = "relation")
female_match <- paste(paste("\\b", female_words$gender,"\\b", sep = ""), collapse="|") #regex friendly 
keep_female <- append(gender_words$gender, c("coloredsoldi", "negrosoldi", "white", "negro"))


S32_long <- data %>% select(long) %>% filter(!is.na(long))
S32_long <- tibble(nrow=1:nrow(S32_long), text = S32_long$long)

long_bigrams_female <- S32_long %>% unnest_tokens(bigram, text, token = "ngrams", n=2) %>% 
  count(bigram, sort =TRUE) %>% filter(grepl(female_match, bigram)) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% filter(stop_words, word != "like" & word != "with" & word != "liked")$word) %>%
  filter(!word2 %in% filter(stop_words, word != "like" & word != "with" & word != "liked")$word)  %>% 
  mutate(word1 = textstem::lemmatize_words(word1), word2 = textstem::lemmatize_words(word2)) %>%
  mutate(word1 = wordStem(word1), word2 = wordStem(word2)) %>%
  count(word1, word2, sort = TRUE) 
long_bigrams_female <- long_bigrams_female[!(long_bigrams_female$word1 == "like" & !long_bigrams_female$word2 %in% keep_female),]
long_bigrams_female <- long_bigrams_female[!(long_bigrams_female$word2 == "like" & !long_bigrams_female$word1 %in% keep_female),]

visualize_bigrams(long_bigrams_female, "Soldiers' Long Response - Female Words Bigrams")
```

### Spatial Arrangement


```{r, spatial , include= F}

# Visualizing Bigrams functions
# From tidytext documentation
bigrams_spatial <- function(data, dictionary, count = 0)
data %>% unnest_tokens(bigram, text, token = "ngrams", n=2) %>%
count(bigram, sort =TRUE) %>% filter(grepl(dictionary, bigram)) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% custom_stop) %>%
filter(!word2 %in% custom_stop) %>%
mutate(word1 = stem_words(lemmatize_words(word1)), word2 = stem_words(lemmatize_words(word2))) %>%
#mutate(word1 = wordStem(word1), word2 = wordStem(word2)) %>%
count(word1, word2, sort = TRUE) %>%
filter(n >= count)

visualize_bigrams <- function(bigrams, title) {
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

bigrams %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void() + theme(legend.position = "none",
plot.title = element_text(hjust = 0.5)) +
ggtitle(title)
}

cooccur_spatial <- function(data, dictionary,  n=5, corr=.15){
dictionary <- stem_words(lemmatize_words(dictionary))
data %>%
mutate(section = row_number()) %>%
filter(section > 0) %>%
unnest_tokens(word, text) %>%
filter(!word %in% custom_stop) %>%
mutate(word = stem_words(lemmatize_words(word))) %>%
group_by(word) %>%
filter(n() >= n) %>%
pairwise_cor(word, section, sort = TRUE) %>%
filter(grepl(dictionary, item1))%>%
filter(correlation > corr)
}


visualize_cooccur <- function(data, title){
set.seed(2020)
data %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void() + theme(legend.position = "none",
plot.title = element_text(hjust = 0.5)) +
ggtitle(title)
}



conn <- dbConnect(drv = PostgreSQL(),
dbname = "sdad",
host = "10.250.124.195",
port = 5432,
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
# query the bipartite edgelist data from github data
data <- dbGetQuery(conn, "SELECT * FROM american_soldier.survey_32_clean")
dbDisconnect(conn)




#alterations to data
##### some data cleaning
#remove possessive 's
data$outfits_comment <- str_replace_all(data$outfits_comment, "'s", "")
data$long <- str_replace_all(data$long, "'s", "")

data$outfits_comment <- str_replace_all(data$outfits_comment, "\\+", "and")
data$long <- str_replace_all(data$long, "\\+", "and")

# some abbreviations to fix
data$outfits_comment <- str_replace_all(data$outfits_comment, "\\bm.p\\b", "mp")
data$long <- str_replace_all(data$long, "\\bm.p\\b", "mp")
data$outfits_comment <- str_replace_all(data$outfits_comment, "\\bp.x\\b", "px")
data$long <- str_replace_all(data$long, "\\bp.x\\b", "px")

#remove all symbols except periods.
data$outfits_comment <- str_replace_all(data$outfits_comment, "[^[:alnum:][:space:].]", "")
data$long <- str_replace_all(data$long, "[^[:alnum:][:space:].]", "")


####collapse certain two word phrases like white man --> whiteman
#read the csv file of correct spellings back in.
collapse <- fread(here("/data/dictionary/collapse_words.csv"), sep = ",") # (n=274)
collapse <- mutate(collapse, original = paste("\\b", original,"\\b", sep = "")) #so that stringr doesn't pick up on instances where it is part of another word

#replace with collapsed words
data$long <- stri_replace_all_regex(data$long, collapse$original, collapse$collapse, vectorize_all = FALSE)
data$outfits_comment <- stri_replace_all_regex(data$outfits_comment, collapse$original, collapse$collapse, vectorize_all = FALSE)
#remove these words:
word_remove = c("question", "questionnaire", "answer")
data$long <- stri_replace_all_regex(data$long, word_remove, "", vectorize_all = FALSE)
data$outfits_comment <- stri_replace_all_regex(data$outfits_comment, word_remove, "", vectorize_all = FALSE)




#load dictionary csv
spatial_words <- fread(here("/data/dictionary/spatial_arrangement.csv"), sep = ",")
space_match <- paste(paste("\\b", spatial_words$space,"\\b", sep = ""), collapse="|") #regex friendly


#### create custom stop words
stop_words <- stop_words %>% filter(word != " no" & word != "with" & word != "cannot" & word != "not"&
                                      word != "different"& word != "differently" & word !="against"& word !="same"
                                    & word !="between"& word !="together"& word !="apart"& word !="place"& word !="places") #
custom_stop <- append(stop_words$word, c("question", "questionnaire", "answer"))


#### words to keep with train
keep_space <- c(collapse$collapse, spatial_words$space, "soldier", "with")




#try removing stop words before everthing....
# data$long <- stri_replace_all_regex(data$long, paste("\\b", custom_stop,"\\b", sep = ""), "", vectorize_all = FALSE)
# data$outfits_comment <- stri_replace_all_regex(data$outfits_comment, paste("\\b", custom_stop,"\\b", sep = ""), "", vectorize_all = FALSE)


#subset data based on question and race
S32W_short <- data %>% filter(racial_group== "white") %>% select(outfits_comment) %>% filter(!is.na(outfits_comment))
S32W_short <- tibble(nrow=1:nrow(S32W_short), text = S32W_short$outfits_comment)

segregation <- filter(data, outfits == "['They should be in separate outfits']")
integration  <- filter(data, outfits == "['They should be together in the same outfits']")
segregation_short <- segregation %>% select(outfits_comment) %>% filter(!is.na(outfits_comment))
seg_short <- tibble(nrow=1:nrow(segregation_short), text = segregation_short$outfits_comment)
integration_short <- integration %>% select(outfits_comment) %>% filter(!is.na(outfits_comment))
int_short <- tibble(nrow=1:nrow(integration_short), text = integration_short$outfits_comment)


S32W_long <- data %>% filter(racial_group== "white") %>% select(long) %>% filter(!is.na(long))
S32W_long <- tibble(nrow=1:nrow(S32W_long), text = S32W_long$long)

S32N_long <- data %>% filter(racial_group== "black") %>% select(long) %>% filter(!is.na(long))
S32N_long <- tibble(nrow=1:nrow(S32N_long), text = S32N_long$long)

```
  


* Spatial segregation of Black Americans was enforced in the American South with numerous local and state laws known as the Jim Crow laws from late 19th century to early 20th century.
* Are themes of spatial segregation prevelant in the soldiers' responses and experiences in the military during WWII?
* We see that there's a greater amount of commentary from black soldiers than white soldiers.
* Black soldiers mention various military spaces and these words are paired with words such as "mix", "together", "same", "separate", and "individual."
* The white soldiers also have commentary on the integration suggests they want separate messhalls, and to not eat or live with the Black soldiers.
* It seems some white soldiers were okay with integration, but these bigrams maybe preceeded by negating words.

```{r, warning=FALSE, message=FALSE, fig.width=12, fig.height=8}

black_long_bigrams <- bigrams_spatial(S32N_long, space_match, count = 2)
visualize_bigrams(black_long_bigrams, "Black Soldiers' Long Response - Spatial Arrangement Bigrams")


white_long_bigrams <- bigrams_spatial(S32W_long, space_match, count = 2)
visualize_bigrams(white_long_bigrams, "Wlack Soldiers' Long Response - Spatial Arrangement Bigrams")

```


* We also looked at how white soldiers talked about spatial arrangments by dorting if they were pro integration of outfits or opposed.
* Although some white soldiers were in favor of integrating outfits, there was still clear commentary from the soldiers should not be sharing the space in living arrangements or social space with black soldiers.
* Also see commentary suggesting Southern and Northern soldiers be seperated.
* Also mentions the Chinese and Filipinos. 




```{r echo = FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=8}
seg_short_cors <- cooccur_spatial(seg_short, space_match, n = 2, corr = .10) 
seg_short_cors <- seg_short_cors[!(seg_short_cors$item1 == "train" & !seg_short_cors$item2 %in% keep_space),]
seg_short_cors <- seg_short_cors[!(seg_short_cors$item2 == "train" & !seg_short_cors$item1 %in% keep_space),]


#plot
visualize_cooccur(seg_short_cors, "Pro Segregation Short Response - Spatial Arrangement Co-occurrences")

int_short_cors <- cooccur_spatial(int_short, space_match, n = 0, corr = 0) 
int_short_cors <- int_short_cors[!(int_short_cors$item1 == "train" & !int_short_cors$item2 %in% keep_space),]
int_short_cors <- int_short_cors[!(int_short_cors$item2 == "train" & !int_short_cors$item1 %in% keep_space),]

#plot
visualize_cooccur(int_short_cors, "Pro Integration Short Response - Spatial Arrangement Co-occurrences")

```

