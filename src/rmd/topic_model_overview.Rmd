---
title: "American Soldier - Topic Modeling"
author: "Morgan Stockham"
date: "6/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(data.table)
library(tidytext)
library(textstem)
library(SnowballC)
library(readxl)
library(rvest)
library(tidytext)
library(dplyr)
library(tm)
library(topicmodels)
library(ggplot2)
```

```{r data, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
S32W <- as.data.table(read_excel("~/Downloads/Survey_32N and 32W consolidated.xlsx", sheet = 1, col_names = TRUE))
S32N <- as.data.table(read_excel("~/Downloads/Survey_32N and 32W consolidated.xlsx", sheet = 2, col_names=TRUE))

text77_df <- tibble(row = 1:nrow(S32W), text = S32W$T3) #Written response to "should soldiers be in separate outfits?"
text78_df <- tibble(row = 1:nrow(S32W), text = S32W$T4) #Written response on overall thoughts on the survey
textn_df <- tibble(row = 1:nrow(S32N), text = S32N$T5) #Written response to "should soldiers be in separate outfits?"
data(stop_words)

tidy_77 <- text77_df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(word = wordStem(word)) %>%
  group_by(row) %>%
  count(word, sort = T)

tidy_78 <- text78_df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(word = wordStem(word)) %>%
  group_by(row) %>%
  count(word, sort = T)

tidy_n <- textn_df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(word = wordStem(word)) %>%
  group_by(row) %>%
  count(word, sort = T)

tidy_77 <- na.omit(tidy_77)
tidy_78 <- na.omit(tidy_78)
tidy_n <- na.omit(tidy_n)

n_words <- textn_df %>%
  unnest_tokens(word, text) %>%
  count(row, sort = T)
words_77 <- text77_df %>%
  unnest_tokens(word, text) %>%
  count(row, sort = T)
words_78 <- text78_df %>%
  unnest_tokens(word, text) %>%
  count(row, sort = T)
```

## Average Responses

The average responses for black soldiers and white soldiers on questions 77 and 78 are a small size for LDA analysis.  The average response for black soldiers for their thoughts on the survey is about 70 words. The average for white soldiers on the short response is 7 and the average for their thoughts on the entire survey are about 47 words. Therefore, analysis using Topic Modelling will be less powerful than other methods.
```{r table, echo = TRUE, message=FALSE, warning=FALSE}
summary(n_words$n)
summary(words_77$n)
summary(words_78$n)
```
##Topic Modelling Graphs
From these graphs we can see that there is a lot of overlap in topics across the three questions and two groups. The following graphs show the density of topics within three clusters of topics across each group and question. There is quite a bit of overlap between them because of the short responses.

```{r dtm, echo = FALSE, results='hide', message=FALSE, warning=FALSE}
dtm_77 <- cast_dtm(tidy_77, term = word, document = row, value = n)
dtm_78 <- cast_dtm(tidy_78, term = word, document = row, value = n)
dtm_n <- cast_dtm(tidy_n, term = word, document = row, value = n)

# lda - mo ---------------------------------------------------------------
# LDA finds topics depending on the number of clusters you want
# number of clusters we want
num_clusters <- 3
lda_77 <- LDA(dtm_77, k = num_clusters, method = "VEM", control = NULL)
lda_78 <- LDA(dtm_78, k = num_clusters, method = "VEM", control = NULL)
lda_n <- LDA(dtm_n, k = num_clusters, method = "VEM", control = NULL)
# this will separate out topics and have a weighted probability
topics_77 <- tidy(lda_77, matrix = "beta")
topics_78 <- tidy(lda_78, matrix = "beta")
topics_n <- tidy(lda_n, matrix = "beta")


# this groups by topics and shows top 10 words and arranges by beta
# Q77 white
topics_terms_77 <- topics_77 %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
# S32 Q78 white
topics_terms_78 <- topics_78 %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
# S32 black
topics_terms_n <- topics_n %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

topics_terms_77 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  xlab("Topic Word Density") +
  ylab("Term") +
  labs(title = "LDA Topic Density for White Soldiers' Q 77") +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
topics_terms_78 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  xlab("Topic Word Density") +
  ylab("Term") +
  labs(title = "LDA Topic Density for White Soldiers' Q 77") +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
topics_terms_n %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  xlab("Topic Word Density") +
  ylab("Term") +
  labs(title = "LDA Topic Density for Black Soldiers' Q 78") +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# exposure_78 <- posterior(lda_78,dtm_78)
# apply(exposure_78$topics,1,sum)
# exposure_n <- posterior(lda_n,dtm_n)
# apply(exposure_n $topics,1,sum)

# euclidean_distances <- c()
# max_exposure <- matrix(F,nrow(exposure_n$topics),num_clusters)
# for(i in 1:nrow(exposure_n$topics)){
#   euclidean_distances[i] <- sqrt(sum((exposure_n$topics[i,] - exposure_78$topics[i,])^2))
#   # which text was exposed to (full v summary)
#   max_exposure[i,which.max(exposure_n$topics[i,])] <- T
#   max_exposure[i,which.max(exposure_78$topics[i,])] <- T
# }
```

## Euclidean Distance Measures

Through the production of a euclidean distance measure we can see the difference between white and black soldiers is $.122$. This determines that the difference between topics of black soldiers and white soldiers for their thoughts on the entire survey is small.
```{r euclid, echo=FALSE, message=FALSE, warning=FALSE}

# print(sum(apply(max_exposure,1,sum) == 1)/nrow(exposure_n$topics))

print(0.1220998)

```

